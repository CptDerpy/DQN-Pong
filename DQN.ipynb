{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qywjCHpxVFMe",
        "colab_type": "text"
      },
      "source": [
        "# Neural networks\n",
        "Classes for DQN and Dueling DQN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwl4X4BmVPaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from random import random, randrange\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "DEEP Q-NETWORK\n",
        "\"\"\"\n",
        "class DQN(nn.Module):\n",
        "    \n",
        "    def __init__(self, obs_shape, n_outputs, device):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.ch, self.w, self.h = obs_shape\n",
        "        self.n_outputs = n_outputs\n",
        "        self.device = device\n",
        "        \n",
        "        # Convolution layers\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=self.ch, \n",
        "                out_channels=32,\n",
        "                kernel_size=8,\n",
        "                stride=4\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=32, \n",
        "                out_channels=64,\n",
        "                kernel_size=4,\n",
        "                stride=2\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=64, \n",
        "                out_channels=64,\n",
        "                kernel_size=3,\n",
        "                stride=1\n",
        "            ),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # Fully connected layers\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.conv_out, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_outputs)\n",
        "        )\n",
        "        \n",
        "    @property\n",
        "    def conv_out(self):\n",
        "        x = self.conv(torch.zeros(1, self.ch, self.w, self.h))\n",
        "        return x.view(1, -1).size(1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "    \n",
        "    def get_action(self, obs, eps=0):\n",
        "        # Epsilon-greedy policy\n",
        "        if random() < eps:\n",
        "            action = randrange(self.n_outputs)\n",
        "        else:\n",
        "            obs = torch.FloatTensor(obs).unsqueeze(0).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                q_val = self.forward(obs)\n",
        "                action = q_val.max(1)[1].item()\n",
        "        return action\n",
        "    \n",
        "    def loss(self, action_values, target_values):\n",
        "        # Mean Squared Error\n",
        "        # return F.mse_loss(action_values, target_values)\n",
        "        # Huber loss\n",
        "        return F.smooth_l1_loss(action_values, target_values)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "DUELING NETWORK\n",
        "\"\"\"\n",
        "class DuelingDQN(nn.Module):\n",
        "    \n",
        "    def __init__(self, obs_shape, n_outputs, device):\n",
        "        super(DuelingDQN, self).__init__()\n",
        "\n",
        "        self.ch, self.w, self.h = obs_shape\n",
        "        self.n_outputs = n_outputs\n",
        "        self.device = device\n",
        "        \n",
        "        # Convolution layers:\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=self.ch, \n",
        "                out_channels=32,\n",
        "                kernel_size=8,\n",
        "                stride=4\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=32, \n",
        "                out_channels=64,\n",
        "                kernel_size=4,\n",
        "                stride=2\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=64, \n",
        "                out_channels=64,\n",
        "                kernel_size=3,\n",
        "                stride=1\n",
        "            ),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # State value function\n",
        "        self.fc_V = nn.Sequential(\n",
        "            nn.Linear(self.conv_out, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "        \n",
        "        # Action advantage function\n",
        "        self.fc_A = nn.Sequential(\n",
        "            nn.Linear(self.conv_out, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_outputs)\n",
        "        )\n",
        "        \n",
        "    @property\n",
        "    def conv_out(self):\n",
        "        x = self.conv(torch.zeros(1, self.ch, self.w, self.h))\n",
        "        return x.view(1, -1).size(1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        adv = self.fc_A(x)\n",
        "        val = self.fc_V(x).expand(x.size(0), self.n_outputs)\n",
        "        x = val + adv - adv.mean(1).unsqueeze(1).expand(x.size(0), self.n_outputs)\n",
        "        return x\n",
        "    \n",
        "    def get_action(self, obs, eps=0):\n",
        "        # Epsilon-greedy policy\n",
        "        if random() < eps:\n",
        "            action = randrange(self.n_outputs)\n",
        "        else:\n",
        "            obs = torch.FloatTensor(obs).unsqueeze(0).to(self.device)\n",
        "            q_val = self.forward(obs)\n",
        "            action = q_val.max(1)[1].item()\n",
        "        return action\n",
        "    \n",
        "    def loss(self, action_values, target_values):\n",
        "        # Mean Squared Error\n",
        "        # return F.mse_loss(action_values, target_values)\n",
        "        # Huber loss\n",
        "        return F.smooth_l1_loss(action_values, target_values)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pkp1XfabVW9v",
        "colab_type": "text"
      },
      "source": [
        "# Experience replay\n",
        "Classes for regular experience replay and prioritised experience replay."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYS958ntVZ5_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "EXPERIENCE REPLAY\n",
        "\"\"\"\n",
        "class ExperienceReplay:\n",
        "    \n",
        "    def __init__(self, buffer_size):\n",
        "        self.buffer = deque(maxlen=buffer_size)\n",
        "    \n",
        "    def add(self, obs, reward, action, obs2, done):\n",
        "        self.buffer.append((obs, reward, action, obs2, done))\n",
        "        \n",
        "    def sample_minibatch(self, batch_size):\n",
        "        mb_idxs = np.random.choice(len(self.buffer), size=batch_size, replace=False)\n",
        "        \n",
        "        mb = zip(*[self.buffer[i] for i in mb_idxs])\n",
        "        obs, reward, action, obs2, done = mb\n",
        "        return (\n",
        "            np.array(obs), \n",
        "            np.array(reward, dtype=np.float32), \n",
        "            np.array(action), \n",
        "            np.array(obs2), \n",
        "            np.array(done, dtype=np.uint8)\n",
        "        )\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "PRIORITIZED EXPERIENCE REPLAY\n",
        "\"\"\"\n",
        "class PER:\n",
        "    \n",
        "    def __init__(self, buffer_size):\n",
        "        self.buffer = deque(maxlen=buffer_size)\n",
        "        self.priorities = deque(maxlen=buffer_size)\n",
        "    \n",
        "    def add(self, obs, reward, action, obs2, done):\n",
        "        self.buffer.append((obs, reward, action, obs2, done))\n",
        "        self.priorities.append(max(self.priorities, default=1))\n",
        "\n",
        "    def get_probabilities(self, priority_scale):\n",
        "        scaled_priorities = np.array(self.priorities) ** priority_scale\n",
        "        batch_probabilities = scaled_priorities / sum(scaled_priorities)\n",
        "        return batch_probabilities\n",
        "\n",
        "    def get_importance(self, probabilities):\n",
        "        importance = 1/len(self.buffer) * 1/probabilities\n",
        "        normalized_importance = importance / max(importance)\n",
        "        return normalized_importance\n",
        "        \n",
        "    def sample_minibatch(self, batch_size, priority_scale=1):\n",
        "        probabilities = self.get_probabilities(priority_scale)\n",
        "\n",
        "        mb_idxs = np.random.choice(len(self.buffer), size=batch_size, replace=False, p=probabilities)\n",
        "        \n",
        "        mb = zip(*[self.buffer[i] for i in mb_idxs])\n",
        "\n",
        "        importance = self.get_importance(probabilities[mb_idxs])\n",
        "\n",
        "        obs, reward, action, obs2, done = mb\n",
        "        return (\n",
        "            np.array(obs), \n",
        "            np.array(reward, dtype=np.float32), \n",
        "            np.array(action), \n",
        "            np.array(obs2), \n",
        "            np.array(done, dtype=np.uint8),\n",
        "            importance,\n",
        "            mb_idxs\n",
        "        )\n",
        "    \n",
        "    def set_priorities(self, idxs, errors, offset=0.1):\n",
        "        for idx, err in zip(idxs, errors):\n",
        "            self.priorities[idx] = abs(err) + offset\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTurjgMWVfLE",
        "colab_type": "text"
      },
      "source": [
        "# DQN\n",
        "To view TensorBoard graphs run \n",
        "\n",
        "\n",
        "```\n",
        "tensorboard --logdir=runs\n",
        "```\n",
        "from the directory where the runs folder is created, and go to localhost:6006 in your web browser.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BObSahHrVhjv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "26000137-d8e3-4805-c287-30ea49fa5ab2"
      },
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "from gym.wrappers import AtariPreprocessing, FrameStack\n",
        "from torch import FloatTensor, LongTensor\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "def optimize(eps):\n",
        "    # Sample minibatch\n",
        "    if PRIORITIZED_REPLAY:\n",
        "        mb_obs, mb_reward, mb_action, mb_obs2, mb_done, importance, idxs = memory.sample_minibatch(BATCH_SIZE, 0.7)\n",
        "    else:\n",
        "        mb_obs, mb_reward, mb_action, mb_obs2, mb_done = memory.sample_minibatch(BATCH_SIZE)\n",
        "    mb_obs = Variable(FloatTensor(mb_obs)).to(device)\n",
        "    mb_reward = Variable(FloatTensor(mb_reward)).to(device)\n",
        "    mb_action = Variable(LongTensor(mb_action)).to(device)\n",
        "    mb_obs2 = Variable(FloatTensor(mb_obs2)).to(device)\n",
        "    mb_done = Variable(FloatTensor(mb_done)).to(device)\n",
        "\n",
        "    # Calculate action values\n",
        "    qv = policy_net.forward(mb_obs).gather(1, mb_action.unsqueeze(1)).squeeze()\n",
        "\n",
        "    # Calculate target values\n",
        "    if DDQN:\n",
        "        mb_target_qv = target_net.forward(mb_obs2).gather(1, policy_net.forward(mb_obs2).max(1)[1].unsqueeze(1)).squeeze()\n",
        "    else:\n",
        "        mb_target_qv = target_net.forward(mb_obs2).max(1)[0]\n",
        "    \n",
        "    # Calculate expected values\n",
        "    e_qv = mb_reward + DISCOUNT * mb_target_qv * (1 - mb_done)\n",
        "\n",
        "    if PRIORITIZED_REPLAY:\n",
        "        # Calculate TD-errors\n",
        "        errors = (e_qv - qv).cpu().detach().numpy()\n",
        "\n",
        "        # Update transition priorities\n",
        "        memory.set_priorities(idxs, errors)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = (FloatTensor(importance**(1 - eps)).to(device) * policy_net.loss(qv, e_qv)).mean() if PRIORITIZED_REPLAY else policy_net.loss(qv, e_qv)\n",
        "\n",
        "    # Update gradient\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def train():\n",
        "    # Initialize environment\n",
        "    step_count = 0\n",
        "    running_loss = 0\n",
        "    batch_reward = []\n",
        "    eps = EPSILON_START\n",
        "    decay = (EPSILON_START - EPSILON_END) / EPSILON_DECAY\n",
        "\n",
        "    # Run episodes\n",
        "    for episode in range(EPISODES):\n",
        "        print('\\rEpisode', episode, end='', flush=True)\n",
        "        \n",
        "        obs = env.reset()\n",
        "        game_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Retrieve action from epsilon-greedy policy\n",
        "            action = policy_net.get_action(obs, eps)\n",
        "            new_obs, reward, done, _ = env.step(action)\n",
        "\n",
        "            # Store transition in replay memory\n",
        "            memory.add(obs, reward, action, new_obs, done)\n",
        "\n",
        "            obs = new_obs\n",
        "            game_reward += reward\n",
        "            step_count += 1\n",
        "\n",
        "            # Epsilon decay\n",
        "            if eps > EPSILON_END:\n",
        "                eps -= decay\n",
        "\n",
        "            # Optimize policy network\n",
        "            if len(memory) > MIN_MEMORY_SIZE:\n",
        "                running_loss += optimize(eps)\n",
        "                \n",
        "                # Update TensorBoard loss graph\n",
        "                if step_count % 1000 == 0:\n",
        "                    writer.add_scalar('training loss', running_loss/1000, step_count)\n",
        "                    running_loss = 0\n",
        "\n",
        "                # Update target network\n",
        "                if step_count % UPDATE_TARGET == 0:\n",
        "                    target_net.load_state_dict(policy_net.state_dict())\n",
        "        \n",
        "        # Update TensorBoard train reward graph\n",
        "        writer.add_scalar('training reward', game_reward, step_count)\n",
        "\n",
        "        # Test target network\n",
        "        if episode % TEST_FREQUENCY == 0:\n",
        "            mean_test_reward = test()\n",
        "            print(f'\\n = | Episode: {episode:4d} | Step: {step_count:7d} | Reward: {mean_test_reward:2.2f} |\\n')\n",
        "            \n",
        "            # Update Tensorboard test reward graph\n",
        "            writer.add_scalar('test reward', mean_test_reward, step_count)\n",
        "\n",
        "            # Save target network parameters to file\n",
        "            torch.save(target_net.state_dict(), f'{log_path}/target_net_params.pt')\n",
        "\n",
        "\n",
        "def test():\n",
        "    print('\\n\\nTesting target network')\n",
        "\n",
        "    games_reward = 0\n",
        "\n",
        "    for episode in range(10):\n",
        "        print('\\r > Test episode', episode, end='', flush=True)\n",
        "\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = target_net.get_action(obs)\n",
        "            obs, reward, done, _ = env.step(action)\n",
        "            games_reward += reward\n",
        "    \n",
        "    return games_reward / 10\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # Hyperparameters\n",
        "    ENV_NAME            =   'PongNoFrameskip-v4'\n",
        "    RUN_TITLE           =   'PER_Dueling_DDQN'\n",
        "    LEARNING_RATE       =   1e-4\n",
        "    EPISODES            =   1000\n",
        "    DISCOUNT            =   0.99\n",
        "    BATCH_SIZE          =   32\n",
        "    MEMORY_SIZE         =   int(1e5)\n",
        "    MIN_MEMORY_SIZE     =   int(1e4)\n",
        "    UPDATE_TARGET       =   int(1e3)\n",
        "    FRAMES_NUMBER       =   4\n",
        "    TEST_FREQUENCY      =   10\n",
        "    EPSILON_START       =   1\n",
        "    EPSILON_END         =   0.1\n",
        "    EPSILON_DECAY       =   int(5e5)\n",
        "    DDQN                =   True\n",
        "    DUELING_DQN         =   True\n",
        "    PRIORITIZED_REPLAY  =   True\n",
        "\n",
        "    # GPU availability\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Path to log files\n",
        "    init_time = datetime.isoformat(datetime.now()).replace(':', '-')[:-7]\n",
        "    log_path = f'runs/{ENV_NAME}_{RUN_TITLE}_{init_time}'\n",
        "\n",
        "    # TensorBoard summary writer\n",
        "    writer = SummaryWriter(log_path)\n",
        "\n",
        "    # Hyperparameter log file\n",
        "    with open(f'{log_path}/hyperparams.txt', 'w') as file:\n",
        "        file.write(f\"{RUN_TITLE} Hyperparameters\\n\\n\")\n",
        "        file.write(f\"Learning rate:\\t\\t {LEARNING_RATE}\\n\")\n",
        "        file.write(f\"Episodes:\\t\\t\\t {EPISODES}\\n\")\n",
        "        file.write(f\"Discount:\\t\\t\\t {DISCOUNT}\\n\")\n",
        "        file.write(f\"Batch size:\\t\\t\\t {BATCH_SIZE}\\n\")\n",
        "        file.write(f\"Memory size:\\t\\t {MEMORY_SIZE}\\n\")\n",
        "        file.write(f\"Min. memory size:\\t {MIN_MEMORY_SIZE}\\n\")\n",
        "        file.write(f\"Update target:\\t\\t {UPDATE_TARGET}\\n\")\n",
        "        file.write(f\"Frames num:\\t\\t\\t {FRAMES_NUMBER}\\n\")\n",
        "        file.write(f\"Test Frequenzy:\\t\\t {TEST_FREQUENCY}\\n\")\n",
        "        file.write(f\"Start exploration:\\t {EPSILON_START}\\n\")\n",
        "        file.write(f\"End exploration:\\t {EPSILON_END}\\n\")\n",
        "        file.write(f\"Exploration steps:\\t {EPSILON_DECAY}\\n\")\n",
        "        file.write(f\"Double DQN:\\t\\t\\t {DDQN}\\n\")\n",
        "        file.write(f\"Dueling DQN:\\t\\t {DUELING_DQN}\\n\")\n",
        "        file.write(f\"Prioritized Replay:\\t {PRIORITIZED_REPLAY}\")\n",
        "\n",
        "    # Environment\n",
        "    env = FrameStack(AtariPreprocessing(gym.make(ENV_NAME)), FRAMES_NUMBER)\n",
        "\n",
        "    # Dimensions of observations\n",
        "    obs_dim = env.observation_space.shape\n",
        "\n",
        "    # Amount of actions\n",
        "    n_outputs = env.action_space.n\n",
        "    \n",
        "    # Neural networks\n",
        "    policy_net = DuelingDQN(obs_dim, n_outputs, device).to(device) if DUELING_DQN \\\n",
        "            else DQN(obs_dim, n_outputs, device).to(device)\n",
        "    target_net = DuelingDQN(obs_dim, n_outputs, device).to(device) if DUELING_DQN \\\n",
        "            else DQN(obs_dim, n_outputs, device).to(device)\n",
        "    target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # Replay memory\n",
        "    memory = PER(MEMORY_SIZE) if PRIORITIZED_REPLAY \\\n",
        "        else ExperienceReplay(MEMORY_SIZE)\n",
        "\n",
        "    # Train policy network\n",
        "    train()\n",
        "\n",
        "    # Close environment\n",
        "    env.close()\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 0\n",
            "\n",
            "Testing target network\n",
            " > Test episode 9\n",
            " = | Episode:    0 | Step:     908 | Reward: -21.00 |\n",
            "\n",
            "Episode 10\n",
            "\n",
            "Testing target network\n",
            " > Test episode 9\n",
            " = | Episode:   10 | Step:    9649 | Reward: -21.00 |\n",
            "\n",
            "Episode 20\n",
            "\n",
            "Testing target network\n",
            " > Test episode 9\n",
            " = | Episode:   20 | Step:   18445 | Reward: -21.00 |\n",
            "\n",
            "Episode 21"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-64c701a5b73c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;31m# Train policy network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;31m# Close environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-64c701a5b73c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;31m# Optimize policy network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mMIN_MEMORY_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Update TensorBoard loss graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-64c701a5b73c>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(eps)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# Update gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}